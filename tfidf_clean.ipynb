{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4e653dc-3024-4e7e-af94-203314efc063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec1dc9a-385b-465e-bd8c-af6dde6edc6d",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2300903b-5dd5-44a5-bfb1-ce61b1991402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctly_read_csv(fname):\n",
    "    prep_df = pd.read_csv(fname, converters={\"tokens_rep\": literal_eval, \"tokens\": literal_eval, \"reference\": literal_eval})\n",
    "    return prep_df\n",
    "\n",
    "def add_extra_keywords(df, key_list, n_repeat):\n",
    "    \"\"\"\n",
    "    Takes df and keyword list, adds n_repeat occurances of keyword to new column when found in doc\n",
    "    \"\"\"\n",
    "    df['tokens_rep_extra'] = df['tokens_rep']\n",
    "    for ind in df.index:\n",
    "        match_list = list()\n",
    "        for keyword in key_list:\n",
    "            # Making everything into a bock of text to find broken up keywords\n",
    "            if keyword in \"\".join(df['tokens_rep'][ind]):\n",
    "                match_list.append(keyword)\n",
    "        if len(match_list) > 0:\n",
    "            i = 0\n",
    "            while i < n_repeat:\n",
    "                df['tokens_rep_extra'][ind] = df['tokens_rep_extra'][ind] + match_list\n",
    "                i += 1\n",
    "    return df['tokens_rep_extra']\n",
    "\n",
    "def only_keywords(df, key_list):\n",
    "    \"\"\"\n",
    "    Will keep only keywords\n",
    "    Right now is returning some blanks, doesn't work on all data sets\n",
    "    \"\"\"\n",
    "    df['tokens_rep_only'] = \"\"\n",
    "    for ind in df.index:\n",
    "        match_list = list()\n",
    "        for keyword in key_list:\n",
    "            # Making everything into a bock of text to find broken up keywords\n",
    "            if keyword in \"\".join(df['tokens_rep'][ind]):\n",
    "                count = 0\n",
    "                while count < \"\".join(df['tokens_rep'][ind]).count(keyword):\n",
    "                    match_list.append(keyword)\n",
    "                    count += 1\n",
    "        if len(match_list) > 0:\n",
    "            df['tokens_rep_only'][ind] = match_list\n",
    "        else:\n",
    "            print(df['doc_index'][ind])\n",
    "            df['tokens_rep_only'][ind] = ['this is bad']\n",
    "    return df['tokens_rep_only']\n",
    "\n",
    "def fake_tokenizer(tokens):\n",
    "    return tokens\n",
    "\n",
    "def get_all_rep_token_strings(token_list):\n",
    "    all_rep_token_strings = []\n",
    "    for d in token_list:\n",
    "        all_rep_token_strings.append(''.join(d))\n",
    "    return all_rep_token_strings\n",
    "\n",
    "def get_key_doc_dict(all_rep_token_strings, doc_ind_list):\n",
    "    # key word, list of document ids that contain it\n",
    "    key_doc_dict = {}\n",
    "    for k in all_key_list:\n",
    "        k = k.strip()\n",
    "        if k:\n",
    "            k_list = []\n",
    "            for i in range(len(all_rep_token_strings)):\n",
    "                if k in all_rep_token_strings[i]:\n",
    "                    k_list.append(doc_ind_list[i])\n",
    "            if k_list:\n",
    "                key_doc_dict[k] = k_list\n",
    "    return key_doc_dict\n",
    "\n",
    "def get_doc_to_keyword_lst_dict(key_doc_dict):\n",
    "    # dict with doc_id as key and list of keywords in it\n",
    "    doc_to_keyword_lst_dict = {}\n",
    "    for k,lst in key_doc_dict.items():\n",
    "        for doc_id in lst:\n",
    "            if doc_id in doc_to_keyword_lst_dict:\n",
    "                doc_to_keyword_lst_dict[doc_id].append(k)\n",
    "            else:\n",
    "                doc_to_keyword_lst_dict[doc_id] = [k]\n",
    "    return doc_to_keyword_lst_dict\n",
    "\n",
    "def get_doc_to_docs_with_overlap_dict(doc_to_keyword_lst_dict):\n",
    "    # get list of lists of overlapping keys per document\n",
    "    doc_to_docs_with_overlap_dict = {}\n",
    "    for doc_id, keywords_lst in doc_to_keyword_lst_dict.items():\n",
    "        keyword_set = set(keywords_lst)\n",
    "        for doc_id_next, keywords_lst_next in doc_to_keyword_lst_dict.items():\n",
    "            if doc_id_next != doc_id:\n",
    "                keyword_set_next = set(keywords_lst_next)\n",
    "                intersection = keyword_set.intersection(keyword_set_next)\n",
    "                if intersection:\n",
    "                    if doc_id in doc_to_docs_with_overlap_dict:\n",
    "                        doc_to_docs_with_overlap_dict[doc_id][doc_id_next] = intersection\n",
    "                    else:\n",
    "                        doc_to_docs_with_overlap_dict[doc_id] = {doc_id_next: intersection}\n",
    "    return doc_to_docs_with_overlap_dict\n",
    "\n",
    "def rank_most_similar_documents(input_doc_embedding, all_docs_embeddings):\n",
    "    \"\"\"\n",
    "    Get top n matched documents plus their indexes\n",
    "    Return list of tuples where first position in each tuple is index and second position is the probability\n",
    "    \"\"\"\n",
    "    distances = cosine_similarity(input_doc_embedding, all_docs_embeddings)[0]\n",
    "    results = zip(range(len(distances)), distances)\n",
    "    results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "    res_with_doc_index = []\n",
    "    return results\n",
    "\n",
    "def rank_most_similar_documents_docid(input_doc_embedding, all_docs_embeddings, doc_ids_all_embeddings):\n",
    "    \"\"\"\n",
    "    Get top n matched documents plus their indexes\n",
    "    Return list of tuples where first position in each tuple is index and second position is the probability\n",
    "    \"\"\"\n",
    "    distances = cosine_similarity(input_doc_embedding, all_docs_embeddings)[0]\n",
    "    results = zip(doc_ids_all_embeddings, distances)\n",
    "    results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "    res_with_doc_index = []\n",
    "    return results\n",
    "\n",
    "def rank_all_embeddings(all_embeddings, doc_index_list):\n",
    "    rank_probs_all = []\n",
    "    rank_inds_all = []\n",
    "    for i in range(len(doc_index_list)):\n",
    "        rank_results = rank_most_similar_documents(all_embeddings[i], all_embeddings)\n",
    "        rank_probs = []\n",
    "        rank_inds = []\n",
    "        for ind,prob in rank_results:\n",
    "            if ind != i:\n",
    "                rank_probs.append(prob)\n",
    "                rank_inds.append(doc_index_list[ind])\n",
    "        rank_probs_all.append(rank_probs)\n",
    "        rank_inds_all.append(rank_inds)\n",
    "    return rank_probs_all, rank_inds_all\n",
    "\n",
    "def rank_all_keyword_overlap_embeddings(search_df, doc_overlap_dict, all_embeds, id_to_ind):\n",
    "    rank_probs_all = []\n",
    "    rank_inds_all = []\n",
    "    for i in range(len(search_df['doc_index'])):\n",
    "        # get list of embeddings for just the input document\n",
    "        if search_df['doc_index'][i] in doc_overlap_dict:\n",
    "            overlap_inner_dict = doc_overlap_dict[search_df['doc_index'][i]]\n",
    "            overlap_docs = overlap_inner_dict.keys()\n",
    "            overlap_embeds = []\n",
    "            for doc_id in overlap_docs:\n",
    "                the_embed = all_embeds[id_to_ind[doc_id]].toarray().flatten()\n",
    "                overlap_embeds.append(the_embed)\n",
    "            if overlap_embeds:\n",
    "                overlap_embeds = np.stack(overlap_embeds)\n",
    "                #print(overlap_embeds.shape)\n",
    "                rank_results = rank_most_similar_documents_docid(all_embeds[i], overlap_embeds, overlap_docs)\n",
    "                rank_probs = []\n",
    "                rank_inds = []\n",
    "                for d_id,prob in rank_results:\n",
    "                    rank_probs.append(prob)\n",
    "                    rank_inds.append(d_id)\n",
    "                rank_probs_all.append(rank_probs)\n",
    "                rank_inds_all.append(rank_inds)\n",
    "            else:\n",
    "                rank_probs_all.append([])\n",
    "                rank_inds_all.append([])\n",
    "        else:\n",
    "            rank_probs_all.append([])\n",
    "            rank_inds_all.append([])\n",
    "    return rank_probs_all, rank_inds_all\n",
    "\n",
    "def create_guess_output(rank_probs, rank_inds, doc_ids, THRESH):\n",
    "    \"\"\"\n",
    "    Returns a df that contains doc_id and matching reference docs\n",
    "    THRESH controls threshold of match probability   \n",
    "    \"\"\"\n",
    "    probs_to_save = []\n",
    "    inds_to_save = []\n",
    "    for i in range(len(rank_inds)):\n",
    "        this_ind_probs_to_save = []\n",
    "        this_ind_inds_to_save = []\n",
    "        this_ind = 0\n",
    "        prob = rank_probs[i][this_ind]\n",
    "        while prob >= THRESH:\n",
    "            this_ind_probs_to_save.append(prob)\n",
    "            this_ind_inds_to_save.append(rank_inds[i][this_ind])\n",
    "            this_ind += 1\n",
    "            prob = rank_probs[i][this_ind]\n",
    "        probs_to_save.append(this_ind_probs_to_save)\n",
    "        inds_to_save.append(this_ind_inds_to_save)\n",
    "\n",
    "    # Creating output df with index and match\n",
    "    dict_list = list()\n",
    "    for i in range(len(doc_ids)):\n",
    "        if len(inds_to_save[i]) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            for ind in inds_to_save[i]:\n",
    "                new_row = {'Test' :doc_ids[i], \"Reference\": ind}\n",
    "                dict_list.append(new_row)\n",
    "    output_df = pd.DataFrame(dict_list)\n",
    "    return output_df\n",
    "\n",
    "def check_training_results(training_labels_df, guesses_df):\n",
    "    # This is stupid but it works\n",
    "    output_touples = [tuple(r) for r in guesses_df.to_numpy()]\n",
    "    training_label_touples = [tuple(r) for r in training_labels_df.to_numpy()]\n",
    "    n_correct_guesses = len(set(output_touples) & set(training_label_touples))\n",
    "    n_correct_guesses\n",
    "    n_guesses = len(output_touples)\n",
    "    n_correct_answers = len(training_label_touples)\n",
    "    precision = n_correct_guesses / n_guesses\n",
    "    recall = n_correct_guesses / n_correct_answers\n",
    "    print(\"Correct Guesses: {}\\nNumber of Guesses: {}\\nPrecision: {} \\n Recall: {}\".format(n_correct_guesses, n_guesses, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea32ff68-352f-447b-becc-1b9949bf83d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_cosine_matches(df, tfidf, doc_tokens_col, THRESH):\n",
    "    \"\"\"\n",
    "    Processes df, returns guesses df with given doc_tokens_col, threshold, and tfidf\n",
    "    \"\"\"\n",
    "    \n",
    "    doc_tokens = df[doc_tokens_col]\n",
    "    tfidf_embeddings = tfidf.fit_transform(doc_tokens)\n",
    "    \n",
    "    doc_ids = df['doc_index']\n",
    "    rank_probs, rank_inds = rank_all_embeddings(tfidf_embeddings, doc_ids)\n",
    "    \n",
    "    pickle.dump(tfidf, open('tfidf_model.pkl', 'wb'))\n",
    "    pickle.dump(tfidf_embeddings, open('tfidf_embeddings.pkl', 'wb'))\n",
    "\n",
    "    guesses_output = create_guess_output(rank_probs, rank_inds, doc_ids, THRESH)\n",
    "    return guesses_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d24415c-00c7-4dcd-bee0-51b0d6debd22",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ba5c9bf-a108-40f7-84a2-7811a57a476b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Guesses: 189\n",
      "Number of Guesses: 282\n",
      "Precision: 0.6702127659574468 \n",
      " Recall: 0.13665943600867678\n"
     ]
    }
   ],
   "source": [
    "training_labels = pd.read_csv('other_info/TrainLabel.csv')\n",
    "\n",
    "train_df = correctly_read_csv(\"processed_data.csv\")\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, tokenizer=fake_tokenizer, lowercase=False)\n",
    "\n",
    "training_guess = tfidf_cosine_matches(train_df, tfidf, \"tokens_rep\", .8)\n",
    "\n",
    "check_training_results(training_labels, training_guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f71d2ef-dcd9-4acf-9bd3-5c2da027167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using previous key list to illustrate add_extra_keywords\n",
    "KEYWORD_PATH = 'other_info/Keywords'\n",
    "chem_syn = pd.read_excel(KEYWORD_PATH + \"/02chem.list.xlsx\").fillna(0)\n",
    "crop_syn = pd.read_excel(KEYWORD_PATH + \"/02crop.list.xlsx\").fillna(0)\n",
    "pest_syn = pd.read_excel(KEYWORD_PATH + \"/02pest.list.xlsx\").fillna(0)\n",
    "\n",
    "# chem\n",
    "s_len = chem_syn.shape[0]\n",
    "chem_cols = chem_syn.columns\n",
    "chem_list = list()\n",
    "crop_list = list()\n",
    "pest_list = list()\n",
    "for i in range(s_len):\n",
    "    # find the longest syn.\n",
    "    base_word = chem_syn['synonym1'][i]\n",
    "    for c in chem_cols:\n",
    "        if chem_syn[c][i]!= 0 and len(chem_syn[c][i]) > len(base_word):\n",
    "            base_word = chem_syn[c][i]\n",
    "    chem_list.append(base_word)\n",
    "    \n",
    "\n",
    "# crop\n",
    "s_len = crop_syn.shape[0]\n",
    "crop_cols = crop_syn.columns\n",
    "for i in range(s_len):\n",
    "    # find the longest syn.\n",
    "    base_word = crop_syn['synonym1'][i]\n",
    "    for c in crop_cols:\n",
    "        if crop_syn[c][i]!= 0 and len(crop_syn[c][i]) > len(base_word):\n",
    "            base_word = crop_syn[c][i]\n",
    "    crop_list.append(base_word)\n",
    "            \n",
    "\n",
    "# pest\n",
    "s_len = pest_syn.shape[0]\n",
    "pest_cols = pest_syn.columns\n",
    "for i in range(s_len):\n",
    "    # find the longest syn.\n",
    "    base_word = pest_syn['synonym1'][i]\n",
    "    for c in pest_cols:\n",
    "        if pest_syn[c][i]!= 0 and len(pest_syn[c][i]) > len(base_word):\n",
    "            base_word = pest_syn[c][i]\n",
    "    pest_list.append(base_word)\n",
    "    \n",
    "\n",
    "\n",
    "key_list = (chem_list + crop_list + pest_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dab4130-63bb-48cd-8399-c20608e77d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mh/_lpdnx451xdfrlb7mmty1g8m0000gn/T/ipykernel_19545/882143226.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tokens_rep_extra'][ind] = df['tokens_rep_extra'][ind] + match_list\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Guesses: 205\n",
      "Number of Guesses: 300\n",
      "Precision: 0.6833333333333333 \n",
      " Recall: 0.14822848879248013\n"
     ]
    }
   ],
   "source": [
    "train_df['tokens_rep_extra'] = add_extra_keywords(train_df, key_list, n_repeat = 2)\n",
    "training_guess_2extra = tfidf_cosine_matches(train_df, tfidf, \"tokens_rep_extra\", .8)\n",
    "\n",
    "check_training_results(training_labels, training_guess_2extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e8ca22-9246-421d-8b1b-50c0dac5cc67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
