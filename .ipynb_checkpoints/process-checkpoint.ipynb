{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = os.listdir('competition_data')\n",
    "\n",
    "id_name = []\n",
    "for i in name:\n",
    "    temp = i.split(\".\")[0]\n",
    "    id_name.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1efHsY16pxK0lBD2gYCgCTnv1Swstq771\n",
      "To: C:\\Users\\WangHongWen\\Desktop\\data_mining\\final_project\\ai cup\\data.zip\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1.88G/1.88G [12:42<00:00, 2.46MB/s]\n"
     ]
    }
   ],
   "source": [
    "# from ckiptagger import data_utils\n",
    "# data_utils.download_data_gdown(\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "all_data2 = []\n",
    "from zhon.hanzi import punctuation\n",
    "\n",
    "for info in os.listdir(r'competition_data'):\n",
    "#     print(info) # form: xxx.txt\n",
    "    domain = os.path.abspath(r'competition_data') #獲取資料夾的路徑\n",
    "    info = os.path.join(domain,info)\n",
    "    info = open(info,'r', encoding=\"utf-8\")\n",
    "    temp = info.read()\n",
    "    temp = temp.replace(\"\\n\", \"\")\n",
    "    all_data.append(temp)\n",
    "    for i in punctuation:\n",
    "        temp = temp.replace(i, '')\n",
    "    all_data2.append(temp)\n",
    "    info.close()\n",
    "# all_data -> with punctuations\n",
    "# all_data2  -> without punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_c = []\n",
    "for para in all_data2:\n",
    "    res = para.replace(\"\\x7f\", \"\")\n",
    "    all_data_c.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chem_syn = pd.read_excel(\"other_info/Keywords/02chem.list.xlsx\")\n",
    "crop_syn = pd.read_excel(\"other_info/Keywords/02crop.list.xlsx\")\n",
    "pest_syn = pd.read_excel(\"other_info/Keywords/02pest.list.xlsx\")\n",
    "\n",
    "chem_syn = chem_syn.fillna(0)\n",
    "crop_syn = crop_syn.fillna(0)\n",
    "pest_syn = pest_syn.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_synonyms(para):\n",
    "    \"\"\"\n",
    "    replace all the synonym by the longest one.\n",
    "    \"\"\"\n",
    "    # chem\n",
    "    s_len = chem_syn.shape[0]\n",
    "    chem_cols = chem_syn.columns\n",
    "    for i in range(s_len):\n",
    "        # find the longest syn.\n",
    "        base_word = chem_syn['synonym1'][i]\n",
    "        for c in chem_cols:\n",
    "            if chem_syn[c][i]!= 0 and len(chem_syn[c][i]) > len(base_word):\n",
    "                base_word = chem_syn[c][i]\n",
    "        # replace all syn.\n",
    "        for col in chem_cols:\n",
    "            if chem_syn[col][i] != 0:\n",
    "                para = para.replace(chem_syn[col][i], base_word)\n",
    "                \n",
    "    # crop\n",
    "    s_len = crop_syn.shape[0]\n",
    "    crop_cols = crop_syn.columns\n",
    "    for i in range(s_len):\n",
    "        # find the longest syn.\n",
    "        base_word = crop_syn['synonym1'][i]\n",
    "        for c in crop_cols:\n",
    "            if crop_syn[c][i]!= 0 and len(crop_syn[c][i]) > len(base_word):\n",
    "                base_word = crop_syn[c][i]\n",
    "        # replace all syn.\n",
    "        for col in crop_cols:\n",
    "            if crop_syn[col][i] != 0:\n",
    "                para = para.replace(crop_syn[col][i], base_word)\n",
    "\n",
    "    # pest\n",
    "    s_len = pest_syn.shape[0]\n",
    "    pest_cols = pest_syn.columns\n",
    "    for i in range(s_len):\n",
    "        # find the longest syn.\n",
    "        base_word = pest_syn['synonym1'][i]\n",
    "        for c in pest_cols:\n",
    "            if pest_syn[c][i]!= 0 and len(pest_syn[c][i]) > len(base_word):\n",
    "                base_word = pest_syn[c][i]\n",
    "        # replace all syn.\n",
    "        for col in pest_cols:\n",
    "            if pest_syn[col][i] != 0:\n",
    "                para = para.replace(pest_syn[col][i], base_word)\n",
    "\n",
    "    return para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_all_data = [] # replace all synonyms with the first words\n",
    "\n",
    "for para in all_data2:\n",
    "    res = replace_synonyms(para).replace(\"\\x7f\", \"\")\n",
    "    r_all_data.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:901: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  warnings.warn(\"`tf.nn.rnn_cell.LSTMCell` is deprecated and will be \"\n",
      "C:\\anaconda\\lib\\site-packages\\keras\\engine\\base_layer_v1.py:1684: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n",
      "Exception ignored in: <bound method WS.__del__ of <ckiptagger.api.WS object at 0x0000027E798C5320>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\anaconda\\lib\\site-packages\\ckiptagger\\api.py\", line 65, in __del__\n",
      "    self.model.sess.close()\n",
      "AttributeError: 'WS' object has no attribute 'model'\n",
      "Exception ignored in: <bound method WS.__del__ of <ckiptagger.api.WS object at 0x0000027E6C007160>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\anaconda\\lib\\site-packages\\ckiptagger\\api.py\", line 65, in __del__\n",
      "    self.model.sess.close()\n",
      "AttributeError: 'WS' object has no attribute 'model'\n"
     ]
    }
   ],
   "source": [
    "from ckiptagger import WS, POS, NER\n",
    "ws = WS(\"./data\")\n",
    "pos = POS(\"./data\")\n",
    "ner = NER(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_spilt = ws(r_all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_spilt2 = ws(all_data_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the number of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lebel = pd.read_csv(\"other_info/TrainLabel.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "index: `id_name`\n",
    "\n",
    "raw_text: `all_data_c`\n",
    "\n",
    "tokens_rep: `word_spilt`\n",
    "\n",
    "tokens: `word_spilt2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cols = ['index', 'raw_text', 'tokens_rep', 'tokens']\n",
    "processed_data = pd.DataFrame({'doc_index': id_name, 'raw_text': all_data_c, 'tokens_rep': word_spilt, 'tokens':word_spilt2})\n",
    "processed_data['tokens_num'] = \"\"\n",
    "processed_data['reference'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_len = len(processed_data['doc_index'])\n",
    "for i in range(p_len):\n",
    "    processed_data['tokens_num'][i] = len(processed_data['tokens_rep'][i])\n",
    "    idx = processed_data['doc_index'][i]\n",
    "    processed_data['reference'][i] = list(train_lebel.query('Test == @idx')['Reference'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data.to_csv(\"processed_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
